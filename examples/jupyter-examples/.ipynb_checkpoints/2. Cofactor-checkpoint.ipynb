{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cofactor\n",
    "\n",
    "Liang's extension of Alternating Least Squares Algorithm. [Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence](https://dl.acm.org/doi/10.1145/2959100.2959182)\n",
    "\n",
    "It co-factorizes both user-item interaction matrix and SPPMI matrix(kind of item-item co-occurence matrix) with shared item matrix. It claims that two different matrix reveals different information, thus exploiting both matrix will be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffalo import CFR, CFROption, StreamOptions\n",
    "from buffalo import aux, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_on_learning': True,\n",
       " 'compute_loss_on_training': True,\n",
       " 'early_stopping_rounds': 0,\n",
       " 'save_best': False,\n",
       " 'evaluation_period': 1,\n",
       " 'save_period': 10,\n",
       " 'random_seed': 0,\n",
       " 'validation': {},\n",
       " 'save_factors': False,\n",
       " 'd': 20,\n",
       " 'num_iters': 10,\n",
       " 'num_workers': 1,\n",
       " 'num_cg_max_iters': 3,\n",
       " 'cg_tolerance': 1e-10,\n",
       " 'eps': 1e-10,\n",
       " 'reg_u': 0.1,\n",
       " 'reg_i': 0.1,\n",
       " 'reg_c': 0.1,\n",
       " 'alpha': 8.0,\n",
       " 'l': 1.0,\n",
       " 'optimizer': 'manual_cg',\n",
       " 'model_path': '',\n",
       " 'data_opt': {}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = CFROption().get_default_option() # initialize default Cofactor option\n",
    "opt                                    # Check buffalo/algo/options.py to see further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_opt = StreamOptions().get_default_option()\n",
    "data_opt.data.sppmi = {\"windows\": 5, \"k\": 10}\n",
    "data_opt.input.main = 'data/ml-1m/stream'\n",
    "data_opt.input.uid = 'data/ml-1m/uid'\n",
    "data_opt.input.iid = 'data/ml-1m/iid'\n",
    "data_opt.data.value_prepro = aux.Option({'name': 'OneBased'})\n",
    "data_opt.data.path = './2-cfr.h5py'\n",
    "data_opt.data.internal_data_type = 'matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] 2023-01-19 14:15:45 [stream.py:279] Create database from stream data\n",
      "[INFO    ] 2023-01-19 14:15:45 [stream.py:103] gathering itemids from data/ml-1m/stream...\n",
      "[INFO    ] 2023-01-19 14:15:45 [stream.py:127] Found 3706 unique itemids\n",
      "[INFO    ] 2023-01-19 14:15:45 [stream.py:288] Creating working data...\n",
      "[INFO    ] 2023-01-19 14:15:47 [stream.py:296] Building data part...\n",
      "[INFO    ] 2023-01-19 14:15:47 [base.py:410] Building compressed triplets for rowwise...\n",
      "[INFO    ] 2023-01-19 14:15:47 [base.py:411] Preprocessing...\n",
      "[INFO    ] 2023-01-19 14:15:47 [base.py:414] In-memory Compressing ...\n",
      "[INFO    ] 2023-01-19 14:15:48 [base.py:294] Load triplet files. Total job files: 11\n",
      "[INFO    ] 2023-01-19 14:15:48 [base.py:444] Finished\n",
      "[INFO    ] 2023-01-19 14:15:48 [base.py:410] Building compressed triplets for colwise...\n",
      "[INFO    ] 2023-01-19 14:15:48 [base.py:411] Preprocessing...\n",
      "[INFO    ] 2023-01-19 14:15:48 [base.py:414] In-memory Compressing ...\n",
      "[INFO    ] 2023-01-19 14:15:48 [base.py:294] Load triplet files. Total job files: 11\n",
      "[INFO    ] 2023-01-19 14:15:48 [base.py:444] Finished\n",
      "[INFO    ] 2023-01-19 14:15:48 [stream.py:168] build sppmi (shift k: 10)\n",
      "[INFO    ] 2023-01-19 14:16:11 [stream.py:179] convert from /tmp/tmpub40v49r to /tmp/tmpwzynbnwf\n",
      "[INFO    ] 2023-01-19 14:16:11 [stream.py:182] sppmi nnz: 350162\n",
      "[INFO    ] 2023-01-19 14:16:11 [stream.py:186] Disk-based Compressing...\n",
      "[INFO    ] 2023-01-19 14:16:11 [base.py:339] Dividing into 20 chunks...\n",
      "[INFO    ] 2023-01-19 14:16:11 [base.py:349] Total job files: 20\n",
      "[PROGRESS] 100.00% 0.0/0.0secs 2,290.91it/s\n",
      "[INFO    ] 2023-01-19 14:16:11 [stream.py:311] DB built on ./3-cfr.h5py\n",
      "[INFO    ] 2023-01-19 14:16:11 [cfr.py:59] CFR ({\n",
      "  \"evaluation_on_learning\": true,\n",
      "  \"compute_loss_on_training\": true,\n",
      "  \"early_stopping_rounds\": 0,\n",
      "  \"save_best\": false,\n",
      "  \"evaluation_period\": 1,\n",
      "  \"save_period\": 10,\n",
      "  \"random_seed\": 0,\n",
      "  \"validation\": {},\n",
      "  \"save_factors\": false,\n",
      "  \"d\": 20,\n",
      "  \"num_iters\": 10,\n",
      "  \"num_workers\": 1,\n",
      "  \"num_cg_max_iters\": 3,\n",
      "  \"cg_tolerance\": 1e-10,\n",
      "  \"eps\": 1e-10,\n",
      "  \"reg_u\": 0.1,\n",
      "  \"reg_i\": 0.1,\n",
      "  \"reg_c\": 0.1,\n",
      "  \"alpha\": 8.0,\n",
      "  \"l\": 1.0,\n",
      "  \"optimizer\": \"manual_cg\",\n",
      "  \"model_path\": \"\",\n",
      "  \"data_opt\": {}\n",
      "})\n",
      "[INFO    ] 2023-01-19 14:16:11 [cfr.py:61] Stream Header(6040, 3706, 994169) Validation(6040 samples)\n"
     ]
    }
   ],
   "source": [
    "cofactor = CFR(opt, data_opt=data_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cofactor.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] 2023-01-19 14:16:15 [buffered_data.py:72] Set data buffer size as 67108864(minimum required batch size is 251).\n",
      "[INFO    ] 2023-01-19 14:16:15 [cfr.py:214] Iteration 1: Loss 0.000 Elapsed 0.098 secs\n",
      "[INFO    ] 2023-01-19 14:16:15 [cfr.py:214] Iteration 2: Loss 0.000 Elapsed 0.095 secs\n",
      "[INFO    ] 2023-01-19 14:16:15 [cfr.py:214] Iteration 3: Loss 0.000 Elapsed 0.093 secs\n",
      "[INFO    ] 2023-01-19 14:16:16 [cfr.py:214] Iteration 4: Loss 0.000 Elapsed 0.093 secs\n",
      "[INFO    ] 2023-01-19 14:16:16 [cfr.py:214] Iteration 5: Loss 0.000 Elapsed 0.091 secs\n",
      "[INFO    ] 2023-01-19 14:16:16 [cfr.py:214] Iteration 6: Loss 0.000 Elapsed 0.093 secs\n",
      "[INFO    ] 2023-01-19 14:16:16 [cfr.py:214] Iteration 7: Loss 0.000 Elapsed 0.095 secs\n",
      "[INFO    ] 2023-01-19 14:16:16 [cfr.py:214] Iteration 8: Loss 0.000 Elapsed 0.095 secs\n",
      "[INFO    ] 2023-01-19 14:16:16 [cfr.py:214] Iteration 9: Loss 0.000 Elapsed 0.092 secs\n",
      "[INFO    ] 2023-01-19 14:16:16 [cfr.py:214] Iteration 10: Loss 0.000 Elapsed 0.099 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cofactor.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for user 61, recommendations are  \n",
      "items ['Patriot,_The_(2000)', 'Frequency_(2000)', 'Shanghai_Noon_(2000)'].\n",
      "\n",
      "for user 62, recommendations are  \n",
      "items ['2001:_A_Space_Odyssey_(1968)', 'Bonnie_and_Clyde_(1967)', 'Close_Encounters_of_the_Third_Kind_(1977)'].\n",
      "\n",
      "for user 63, recommendations are  \n",
      "items ['Blair_Witch_Project,_The_(1999)', 'Eyes_Wide_Shut_(1999)', 'Austin_Powers:_The_Spy_Who_Shagged_Me_(1999)'].\n",
      "\n",
      "for user 64, recommendations are  \n",
      "items ['Jurassic_Park_(1993)', 'Terminator_2:_Judgment_Day_(1991)', 'Star_Wars:_Episode_VI_-_Return_of_the_Jedi_(1983)'].\n",
      "\n",
      "for user 65, recommendations are  \n",
      "items ['Braveheart_(1995)', 'Saving_Private_Ryan_(1998)', 'Patriot,_The_(2000)'].\n",
      "\n",
      "for user 66, recommendations are  \n",
      "items ['Jurassic_Park_(1993)', 'Braveheart_(1995)', 'Patriot,_The_(2000)'].\n",
      "\n",
      "for user 67, recommendations are  \n",
      "items ['Bridge_on_the_River_Kwai,_The_(1957)', 'To_Kill_a_Mockingbird_(1962)', 'North_by_Northwest_(1959)'].\n",
      "\n",
      "for user 68, recommendations are  \n",
      "items ['Being_John_Malkovich_(1999)', 'American_Beauty_(1999)', 'Shakespeare_in_Love_(1998)'].\n",
      "\n",
      "for user 69, recommendations are  \n",
      "items ['Good_Will_Hunting_(1997)', 'Dead_Man_Walking_(1995)', 'Apollo_13_(1995)'].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uids = [str(x) for x in range(61, 70)]\n",
    "recommendation_result = cofactor.topk_recommendation(uids, topk=3)\n",
    "for uid, iids in recommendation_result.items():\n",
    "    print(f\"for user {uid}, recommendations are \", f\"\\nitems {iids}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation for users in given pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for user 1, recommendations are  \n",
      "items ['Frequency_(2000)', 'Remember_the_Titans_(2000)', 'Shanghai_Noon_(2000)'].\n",
      "\n",
      "for user 2, recommendations are  \n",
      "items ['Rules_of_Engagement_(2000)', 'Shanghai_Noon_(2000)', 'Remember_the_Titans_(2000)'].\n",
      "\n",
      "for user 3, recommendations are  \n",
      "items ['Shanghai_Noon_(2000)', '28_Days_(2000)', 'Gone_in_60_Seconds_(2000)'].\n",
      "\n",
      "for user 4, recommendations are  \n",
      "items ['Shanghai_Noon_(2000)', 'Skulls,_The_(2000)', 'Gone_in_60_Seconds_(2000)'].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pool = ['Rules_of_Engagement_(2000)', \n",
    "        'Remember_the_Titans_(2000)', \n",
    "        'Skulls,_The_(2000)', \n",
    "        '28_Days_(2000)', \n",
    "        'Frequency_(2000)', \n",
    "        'Gone_in_60_Seconds_(2000)', \n",
    "        'What_Lies_Beneath_(2000)', \n",
    "        'Reindeer_Games_(2000)', \n",
    "        'Final_Destination_(2000)', \n",
    "        'Shanghai_Noon_(2000)']\n",
    "uids = [str(x) for x in range(5)]\n",
    "recommendation_result = cofactor.topk_recommendation(uids, topk=3, pool=pool)\n",
    "for uid, iids in recommendation_result.items():\n",
    "    print(f\"for user {uid}, recommendations are \", f\"\\nitems {iids}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar movies to Toy_Story_2_(1999) in similar items\n",
      "[(\"Bug's_Life,_A_(1998)\", 0.9336268), ('Toy_Story_(1995)', 0.910489), ('Shakespeare_in_Love_(1998)', 0.8634493), ('Babe_(1995)', 0.8460558), ('Groundhog_Day_(1993)', 0.8172974), ('Being_John_Malkovich_(1999)', 0.80523443), ('Sixth_Sense,_The_(1999)', 0.7978962), ('Galaxy_Quest_(1999)', 0.7975167), ('Election_(1999)', 0.79327524), ('South_Park:_Bigger,_Longer_and_Uncut_(1999)', 0.7679639)]\n",
      "01. 0.934 Bug's_Life,_A_(1998)\n",
      "02. 0.910 Toy_Story_(1995)\n",
      "03. 0.863 Shakespeare_in_Love_(1998)\n",
      "04. 0.846 Babe_(1995)\n",
      "05. 0.817 Groundhog_Day_(1993)\n",
      "06. 0.805 Being_John_Malkovich_(1999)\n",
      "07. 0.798 Sixth_Sense,_The_(1999)\n",
      "08. 0.798 Galaxy_Quest_(1999)\n",
      "09. 0.793 Election_(1999)\n",
      "10. 0.768 South_Park:_Bigger,_Longer_and_Uncut_(1999)\n"
     ]
    }
   ],
   "source": [
    "print('Similar movies to Toy_Story_2_(1999) in similar items')\n",
    "similar_items = cofactor.most_similar('Toy_Story_2_(1999)', 10)\n",
    "print(similar_items)\n",
    "for rank, (movie_name, score) in enumerate(similar_items):\n",
    "    print(f'{rank + 1:02d}. {score:.3f} {movie_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most similar items given pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. 0.371 Gone_in_60_Seconds_(2000)\n",
      "02. 0.357 28_Days_(2000)\n",
      "03. 0.335 Frequency_(2000)\n",
      "04. 0.332 Shanghai_Noon_(2000)\n",
      "05. 0.283 What_Lies_Beneath_(2000)\n",
      "06. 0.231 Final_Destination_(2000)\n"
     ]
    }
   ],
   "source": [
    "pool = ['Rules_of_Engagement_(2000)', \n",
    "        'Remember_the_Titans_(2000)', \n",
    "        'Skulls,_The_(2000)', \n",
    "        '28_Days_(2000)', \n",
    "        'Frequency_(2000)', \n",
    "        'Gone_in_60_Seconds_(2000)', \n",
    "        'What_Lies_Beneath_(2000)', \n",
    "        'Reindeer_Games_(2000)', \n",
    "        'Final_Destination_(2000)', \n",
    "        'Shanghai_Noon_(2000)']\n",
    "similar_items = cofactor.most_similar('Toy_Story_2_(1999)', 5, pool=pool)\n",
    "for rank, (movie_name, score) in enumerate(similar_items):\n",
    "    print(f'{rank + 1:02d}. {score:.3f} {movie_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
